10/31/2014 (Happy Halloween!) 

Here is the first result of convolutional FISTA dictionary learning. 
The average reconstruction error is 35% of the input energy of the images. 
The average sparsity is 87.7% (i.e. 12.3% of the units are active). 
Loss = 0.023277 after 40 epochs.

11/06/2014
verified LISTA. Needs a *tiny* learning rate, i.e. 1e-7 
******************************No training, just looking at ISTA iterations**********************
t7> dofile('train_LISTA.lua') 
number of loops: 0  
Initializing convolutional LISTA... 
 [===========================================================>]  62/62 
1 Time: 0.85797381401062 %Rec.Error 0.92135085148757 Sparsity:0.71945488837457 Loss: 6912.77062516  
number of loops: 1  
Initializing convolutional LISTA... 
 [===========================================================>]  62/62 
1 Time: 1.4978051185608 %Rec.Error 0.84834119230085 Sparsity:0.71910421309933 Loss: 6359.0316061338 
number of loops: 3  
Initializing convolutional LISTA... 
 [===========================================================>]  62/62 
1 Time: 2.9268279075623 %Rec.Error 0.71877645857295 Sparsity:0.7171317685035 Loss: 5406.7148435429  

******************************Training LISTA with various number of loops************************
t7> dofile('train_LISTA.lua') 
number of loops: 0  
Initializing convolutional LISTA... 
 [===========================================================>]  62/62 
1 Time: 0.95641803741455 %Rec.Error 0.39870864297675 Sparsity:0.54903325726909 Loss: 2994.7509468209    
 [===========================================================>]  62/62 
2 Time: 0.88221788406372 %Rec.Error 0.26600137707685 Sparsity:0.63207549433554 Loss: 1999.1638147393    
 [===========================================================>]  62/62 
3 Time: 0.88482689857483 %Rec.Error 0.23925748302238 Sparsity:0.67217442297166 Loss: 1794.7308496274    
 [===========================================================>]  62/62 
4 Time: 0.85296392440796 %Rec.Error 0.22497487627547 Sparsity:0.69572159551805 Loss: 1697.3022823348    
 [===========================================================>]  62/62 
5 Time: 0.85844683647156 %Rec.Error 0.21753641273866 Sparsity:0.71417777769027 Loss: 1628.9810500684    
number of loops: 1  
Initializing convolutional LISTA... 
 [===========================================================>]  62/62 
1 Time: 1.5154020786285 %Rec.Error 0.27880384823072 Sparsity:0.64951029131489 Loss: 2102.9658223413 
 [===========================================================>]  62/62 
2 Time: 1.4677839279175 %Rec.Error 0.20802865268845 Sparsity:0.74025200259301 Loss: 1564.1877379419 
 [===========================================================>]  62/62 
3 Time: 1.4843411445618 %Rec.Error 0.19628353991144 Sparsity:0.76847894730106 Loss: 1481.533501079  
 [===========================================================>]  62/62 
4 Time: 1.4476680755615 %Rec.Error 0.18794584357395 Sparsity:0.78588873340238 Loss: 1412.3334280881 
 [===========================================================>]  62/62 
5 Time: 1.4343779087067 %Rec.Error 0.18573516477592 Sparsity:0.79771247986824 Loss: 1388.3802935065 
number of loops: 3  
Initializing convolutional LISTA... 
 [===========================================================>]  62/62 
1 Time: 2.9165000915527 %Rec.Error 0.23509431522351 Sparsity:0.77959451367778 Loss: 1768.4735728511 
 [===========================================================>]  62/62 
2 Time: 2.9075360298157 %Rec.Error 0.17739743895309 Sparsity:0.82337699397918 Loss: 1332.7129613886 
 [===========================================================>]  62/62 
3 Time: 2.8507480621338 %Rec.Error 0.16917258958741 Sparsity:0.83804305907219 Loss: 1272.8065152321 
 [===========================================================>]  62/62 
4 Time: 2.9065687656403 %Rec.Error 0.16653304744402 Sparsity:0.84723377227783 Loss: 1254.7878427589 
 [===========================================================>]  62/62 
5 Time: 2.9119718074799 %Rec.Error 0.16398698916228 Sparsity:0.8537809002784 Loss: 1230.4723144422 

In both cases (with and without training), adding more loops helps achieve a lower loss.   
